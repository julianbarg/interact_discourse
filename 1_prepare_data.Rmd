---
date: "2022-11-08"
output: markdown
---

## Functions

```{r load, include=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(glue)
# library(SnowballC)
library(textstem)
library(tidytext)
library(tm)
options(max.print = 1000)
util_folder <- c("util")
beep <- function(){
  system(str_c("notify-send 'Task complete.'",
               "&& paplay /usr/share/sounds/Yaru/stereo/message.oga"))
}
```

## Input

```{r input}
metadata <- yaml::read_yaml(here("input", "metadata.yaml"))
known_entities <- yaml::read_yaml(here("input", "nlp_input.yaml")) %>%
  pluck("known_entities") %>%
  map_dfr(flatten_dfc) %>%
  {set_names(.$token, regex(.$regex, ignore_case = T))}
custom_stopwords <- yaml::read_yaml(here("input", "nlp_input.yaml")) %>%
  pluck("custom_stopwords")
known_entities
custom_stopwords
```

## Loader

```{r load}
source(here(util_folder, "read_data.R"))
source(here("input", "load_fixes.R"))
source(here("util", "clean_document.R"))

data <- metadata$index %>%
  map(read_data) %>%
  map(~ fixes[[.x$group]](.x)) %>%
  map(~ list_modify(.x, preprocessing = 
                      clean_document(.x$document, known_entities)))
glimpse(data[[11]])
```

## Compounds

4

```{r compounds}
source(here("util", "get_compound_candidates.R"))
source(here("util", "fix_collocations.R"))
compounds_4 <- data %>%
  map(~ .x$preprocessing$content) %>%
  flatten_chr() %>%
  get_compound_candidates(known_entities, custom_stopwords, 
                          size = 4, min_count = 10)
compounds_4
data <- data %>%
  map(~ modify_at(.x, "preprocessing",
                  ~ fix_collocations(.x, compounds_4$collocation)))
cat(compounds_4$collocation, file = here("output", "compounds.txt"),
    sep = "\n", append = T)
```

3

```{r compounds}
compounds_3 <- data %>%
  map(~ .x$preprocessing$content) %>%
  flatten_chr() %>%
  get_compound_candidates(known_entities, custom_stopwords, 
                          size = 3, min_count = 10)
compounds_3
data <- data %>%
  map(~ modify_at(.x, "preprocessing",
                  ~ fix_collocations(.x, compounds_3$collocation)))
cat(compounds_3$collocation, file = here("output", "compounds.txt"),
    sep = "\n", append = T)
```

2

```{r compounds}
compounds_2 <- data %>%
  map(~ .x$preprocessing$content) %>%
  flatten_chr() %>%
  get_compound_candidates(known_entities, custom_stopwords, 
                          size = 2, min_count = 10)
compounds_2
data <- data %>%
  map(~ modify_at(.x, "preprocessing",
                  ~ fix_collocations(.x, compounds_2$collocation)))
cat(compounds_2$collocation, file = here("output", "compounds.txt"),
    sep = "\n", append = T)
```

## Tokenize

```{r tokenize}
source(here(util_folder, "tokenize.R"))
data_tokens <- data %>%
  map( ~ list_modify(.x, tokens = mutate(
    tokenize(.x$preprocessing, custom_stopwords)
  )))
glimpse(data_tokens[[1]])
beep()
```

Sanity check

```{r test}
data_tokens[[11]]$tokens %>%
  filter(str_detect(token, "^tar sand|^oil sand"))
```

# Frequency

1. Infrequent

```{r infrequent}
low_freq <- 0.009
source(here(util_folder, "count_by_doc.R"))
source(here(util_folder, "get_frequency.R"))
frequency <- data_tokens %>%
  map_dfr(count_by_doc) %>%
  get_frequency()
infrequent <- frequency %>%
  filter(share < low_freq)
infrequent %>%
  arrange(desc(frequency), token)
data_frequent <- data_tokens %>%
  map(~ modify_at(.x, "tokens", ~ anti_join(.x, infrequent, by = "token")))

file.remove(here("output", "infrequent.txt"))
file.create(here("output", "infrequent.txt"))
cat(infrequent$token, file = here("output", "infrequent.txt"),
    sep = "\n")
```

2. tfidf 

```{r tfidf}
tfidf_cutoff <- 0.001
source(here(util_folder, "prop_tfidf.R"))
source(here(util_folder, "remove_tfidf.R"))
tfidf <- data_frequent %>%
  map_dfr(count_by_doc) %>%
  prop_tfidf(tfidf_cutoff)
arrange(tfidf, desc(tf_idf))
high_tfidf_data <- data_frequent  %>%
  map(remove_tfidf, tfidf)
glimpse(high_tfidf_data[1:2])
```

Too frequent

```{r frequent_tokens}
too_frequent_cutoff <- 0.15
frequency_2 <- high_tfidf_data %>%
# frequency_2 <- data_frequent %>%
  map_dfr(count_by_doc) %>%
  get_frequency()
frequency_2 %>%
  arrange(desc(frequency))
frequent <- frequency_2 %>%
  filter(share > too_frequent_cutoff)
trimmed_data <- high_tfidf_data %>%
# trimmed_data <- data_frequent %>%
  map(~ modify_at(.x, "tokens", ~ anti_join(.x, frequent, by = "token")))

file.remove(here("output", "frequent.txt"))
file.create(here("output", "frequent.txt"))
cat(frequent$token, file = here("output", "frequent.txt"),
    sep = "\n")
```

## Count tokens

```{r count}
count_tokens <- function(df){
  df %>%
    group_by(order) %>%
    summarize(total_tokens = sum(n), .groups = "drop")
}
data_counted <- trimmed_data %>%
  map(~ list_modify(.x, token_counts = count_tokens(.x$tokens)))
glimpse(data_counted[[11]])
```

## Save

```{r save}
write_rds(data_counted, here("output", "trimmed_data.rds"))
```
